# Homography Configuration
# Controls which approach is used for computing homography transformations

homography:
  # Primary approach: intrinsic_extrinsic | feature_match | learned
  # - intrinsic_extrinsic: Uses camera calibration parameters and PTZ pose
  #   Best for: PTZ cameras with known parameters, controlled environments
  # - feature_match: Uses feature detection (SIFT/ORB/LoFTR) and matching
  #   Best for: Unknown camera parameters, reference images available
  # - learned: Uses neural network models for homography estimation
  #   Best for: Challenging conditions, when training data is available
  approach: intrinsic_extrinsic

  # Fallback approaches (tried in order if primary fails)
  # Provides robustness when primary approach cannot compute valid homography
  fallback_approaches:
    - feature_match

  # ========================================================================
  # Approach-specific configuration
  # ========================================================================

  # Configuration for intrinsic_extrinsic approach
  intrinsic_extrinsic:
    # Physical sensor width in millimeters
    # Used to convert focal length from mm to pixels
    # Default: 7.18mm (typical for 1/2.8" sensors)
    sensor_width_mm: 7.18

    # Base focal length in millimeters at 1x zoom
    # Scaled linearly with zoom factor for computing intrinsics
    # Default: 5.9mm (typical for PTZ cameras)
    base_focal_length_mm: 5.9

    # Map visualization scale: pixels per meter
    # Higher values = more detail but smaller coverage area
    # Default: 100 pixels/meter (1 meter = 100 pixels on map)
    pixels_per_meter: 100.0

  # Configuration for feature_match approach
  feature_match:
    # Feature detector type: sift | orb | loftr
    # - sift: Scale-Invariant Feature Transform (robust, patented)
    # - orb: Oriented FAST and Rotated BRIEF (fast, free)
    # - loftr: Learned feature matcher (requires deep learning)
    detector: sift

    # Minimum number of feature matches required for valid homography
    # Must be at least 4 (theoretical minimum), recommend 10-20 for robustness
    min_matches: 4

    # RANSAC inlier threshold in pixels
    # Points with reprojection error < threshold are considered inliers
    # Typical range: 1.0-5.0 pixels
    ransac_threshold: 5.0

    # ========================================================================
    # Camera Capture Context
    # ========================================================================
    # The camera configuration and PTZ position where GCPs were captured.
    # This allows the validation test to:
    # 1. Move the camera to the exact position where GCPs were collected
    # 2. Grab a frame and overlay GCP points for visual verification
    # 3. Validate that GCPs still align with the physical features
    #
    # Required for visual GCP validation testing.
    # ========================================================================
    camera_capture_context:
      # Camera name (must match an entry in camera_config.py)
      camera_name: "Valte"

      # Image dimensions at capture time
      image_width: 2560
      image_height: 1440

      # PTZ position where GCPs were captured
      # The test will move the camera to this position before validation
      ptz_position:
        pan: 73.9      # Pan angle in degrees (0-360)
        tilt: 24.1     # Tilt angle in degrees (negative = looking down)
        zoom: 1.0       # Zoom factor (1.0 = no zoom)

      # Camera intrinsics (optional - can be computed from zoom + sensor params)
      # If provided, these override the computed values
      intrinsics:
        # Focal length in pixels (fx = fy for square pixels)
        # Can be computed as: (zoom * base_focal_length_mm / sensor_width_mm) * image_width
        focal_length_px: null  # null = compute from zoom

        # Principal point (optical center) in pixels
        # Default: image center
        principal_point:
          cx: null  # null = image_width / 2
          cy: null  # null = image_height / 2

      # Capture timestamp (for reference)
      capture_timestamp: "2025-11-28T10:30:00Z"

      # Notes about capture conditions
      notes: "Clear day, good lighting conditions"

    # Ground Control Points (GCPs) for feature matching and camera pose estimation
    #
    # GCPs are pairs of corresponding points: map pixel coordinates on a reference
    # map and pixel coordinates in the camera image. They establish the relationship
    # between the camera view and the reference map, enabling:
    # - Camera pose estimation (position and orientation)
    # - Image-to-map coordinate transformation
    # - Geometric calibration and validation
    #
    # How to acquire GCPs:
    # 1. Map marking: On the reference map image, identify pixel coordinates of
    #    distinctive features (building corners, landmarks)
    # 2. Pixel marking: In the camera image, manually mark the exact pixel
    #    coordinates (image_u, image_v) corresponding to each map point
    # 3. Quality control: Verify measurements, prefer high-contrast features,
    #    distribute points across the entire image frame
    #
    # Minimum requirements:
    # - At least 6 points recommended for RANSAC robustness
    # - Points should be well-distributed across the image
    # - Avoid collinear or co-planar point configurations
    ground_control_points:
      # GCP 1: Building corner Northwest
      - map_id: "map_valte"
        map_pixel_x: 1250.5
        map_pixel_y: 680.0
        image_u: 1250.5
        image_v: 680.0
        metadata:
          description: "Building corner NW"
          timestamp: "2025-11-28T10:30:00Z"

      # GCP 2: Building corner Northeast
      - map_id: "map_valte"
        map_pixel_x: 2456.2
        map_pixel_y: 695.5
        image_u: 2456.2
        image_v: 695.5
        metadata:
          description: "Building corner NE"
          timestamp: "2025-11-28T10:31:15Z"

      # GCP 3: Road intersection Southwest
      - map_id: "map_valte"
        map_pixel_x: 1180.0
        map_pixel_y: 1365.2
        image_u: 1180.0
        image_v: 1365.2
        metadata:
          description: "Road intersection SW"
          timestamp: "2025-11-28T10:32:45Z"

      # GCP 4: Building corner Southeast
      - map_id: "map_valte"
        map_pixel_x: 2520.8
        map_pixel_y: 1398.8
        image_u: 2520.8
        image_v: 1398.8
        metadata:
          description: "Building corner SE"
          timestamp: "2025-11-28T10:33:20Z"

      # GCP 5: Tree center
      - map_id: "map_valte"
        map_pixel_x: 1680.5
        map_pixel_y: 1120.0
        image_u: 1680.5
        image_v: 1120.0
        metadata:
          description: "Tree center"
          timestamp: "2025-11-28T10:34:00Z"

      # GCP 6: Parking lot marker
      - map_id: "map_valte"
        map_pixel_x: 1920.3
        map_pixel_y: 1088.0
        image_u: 1920.3
        image_v: 1088.0
        metadata:
          description: "Parking lot marker"
          timestamp: "2025-11-28T10:35:30Z"

  # Configuration for learned approach
  learned:
    # Path to trained model weights file
    # Set to null to use default pre-trained model (if available)
    # Format depends on model_type (.pth for PyTorch, .h5 for TensorFlow)
    model_path: null

    # Minimum confidence score for valid homography
    # Range: 0.0-1.0, where 1.0 = highest confidence
    # Higher threshold = more selective, fewer false positives
    confidence_threshold: 0.5
